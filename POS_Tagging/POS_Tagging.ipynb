{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS_Tagging",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVm4Wqd-y_wr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "outputId": "d8948661-0d6e-4d25-8489-00221d596562"
      },
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.3.2)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.1.0)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: en_core_web_md==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.1/en_core_web_md-2.3.1.tar.gz#egg=en_core_web_md==2.3.1 in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.3.1) (2.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (49.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.7.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.1.0)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlMzwQKapT4Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bad03b7a-1083-4408-84be-ca662291c02a"
      },
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "import spacy\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load(parse=True, tag=True, entity=True)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n",
        "stop_words = set(stopword_list)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VjfdAPUp3qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "demo_sites = ['https://inshorts.com/en/read/science',\n",
        "              'https://inshorts.com/en/read/world',\n",
        "              'https://inshorts.com/en/read/technology']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIDpaZbvqvgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_dataset(demo_sites):\n",
        "    news_data = []\n",
        "\n",
        "    for url in demo_sites:\n",
        "        category = url.split('/')[-1]\n",
        "        data = requests.get(url)\n",
        "        soup = BeautifulSoup(data.content, 'html.parser')       \n",
        "        news_articles = [{'headline': headline.find('span', attrs={\"itemprop\": \"headline\"}).string,\n",
        "                          'article': article.find('div', attrs={\"itemprop\": \"articleBody\"}).string}\n",
        "                           for headline, article in \n",
        "                             zip(soup.find_all('div', class_=[\"news-card-title news-right-box\"]),\n",
        "                                 soup.find_all('div', class_=[\"news-card-content news-right-box\"]))\n",
        "                        ]\n",
        "        news_data.extend(news_articles)\n",
        "        \n",
        "    df =  pd.DataFrame(news_data)\n",
        "    df = df[['headline', 'article']]\n",
        "    return df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVxgLJxHsqU8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "082f526f-edc0-4ee5-facf-f7de63ae014a"
      },
      "source": [
        "news_df = generate_dataset(demo_sites)\n",
        "news_df.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Doctors find roundworm in Japanese woman's ton...</td>\n",
              "      <td>After a 25-year-old woman in Tokyo visited the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Moderna COVID-19 vaccine safe, induces immune ...</td>\n",
              "      <td>Moderna's experimental COVID-19 vaccine showed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Moderna to start final-stage COVID-19 vaccine ...</td>\n",
              "      <td>Moderna on Tuesday said it plans to start its ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Researchers make medical wearable using a penc...</td>\n",
              "      <td>The University of Missouri researchers have us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SpaceX seeks to fly its prototype Mars rocket ...</td>\n",
              "      <td>Elon Musk's SpaceX is looking to fly its 'Star...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>UAE postpones launch of Mars orbiter citing we...</td>\n",
              "      <td>The UAE has postponed the launch of its Mars m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NASA delays launch of $10B James Webb Space Te...</td>\n",
              "      <td>NASA has again delayed the launch of its next-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>No, we did not change the zodiac: NASA after r...</td>\n",
              "      <td>After reports of NASA adding a new zodiac sign...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Metal-eating bacteria discovered as scientist ...</td>\n",
              "      <td>Microbiologists from California Institute of T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Scientists recover 'severely' damaged human lu...</td>\n",
              "      <td>Researchers have recovered donated lungs, deem...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline                                            article\n",
              "0  Doctors find roundworm in Japanese woman's ton...  After a 25-year-old woman in Tokyo visited the...\n",
              "1  Moderna COVID-19 vaccine safe, induces immune ...  Moderna's experimental COVID-19 vaccine showed...\n",
              "2  Moderna to start final-stage COVID-19 vaccine ...  Moderna on Tuesday said it plans to start its ...\n",
              "3  Researchers make medical wearable using a penc...  The University of Missouri researchers have us...\n",
              "4  SpaceX seeks to fly its prototype Mars rocket ...  Elon Musk's SpaceX is looking to fly its 'Star...\n",
              "5  UAE postpones launch of Mars orbiter citing we...  The UAE has postponed the launch of its Mars m...\n",
              "6  NASA delays launch of $10B James Webb Space Te...  NASA has again delayed the launch of its next-...\n",
              "7  No, we did not change the zodiac: NASA after r...  After reports of NASA adding a new zodiac sign...\n",
              "8  Metal-eating bacteria discovered as scientist ...  Microbiologists from California Institute of T...\n",
              "9  Scientists recover 'severely' damaged human lu...  Researchers have recovered donated lungs, deem..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIjPDKfNV2Xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text_strip = soup.get_text()\n",
        "    return text_strip\n",
        "\n",
        "def remove_special_characters(text, remove_digit=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digit else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def stemmer(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "def lemmatizer(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    filtered = [token for token in tokens if not token in stop_words]\n",
        "    filtered_text = ' '.join(filtered)\n",
        "    return filtered_text\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvvt22HZtOIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_corpus(corpus, strip_html=True, text_lower_case=True,\n",
        "                     lemmatization=True, remove_special_chars=True,\n",
        "                     remove_stopword=True, remove_digits=True):\n",
        "\n",
        "    normalized_text = []\n",
        "    for text in corpus:\n",
        "        # strip HTML\n",
        "        if strip_html:\n",
        "           text = strip_html_tags(text)\n",
        "        # lowercase the text\n",
        "        if text_lower_case:\n",
        "           text = text.lower()\n",
        "        # remove extra newlines\n",
        "        text = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', text)\n",
        "        # lemmatize text\n",
        "        if lemmatization:\n",
        "            text = lemmatizer(text)\n",
        "        # remove special characters and\\or digits    \n",
        "        if remove_special_chars:\n",
        "            # isolate special characters by inserting spaces    \n",
        "            pattern = re.compile(r'([{.(-)!}])')\n",
        "            text = pattern.sub(\" \\\\1 \", text)\n",
        "            text = remove_special_characters(text, remove_digit=remove_digits)\n",
        "        # remove extra whitespace\n",
        "        text = re.sub(' +', ' ', text)\n",
        "        # remove stopwords\n",
        "        if remove_stopword:\n",
        "           text = remove_stopwords(text)\n",
        "        \n",
        "        normalized_text.append(text)\n",
        "\n",
        "    return normalized_text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYV6rfg07zi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "aadb3453-2920-45b9-eac5-64ec3a75d49a"
      },
      "source": [
        "# combining headline and article text\n",
        "news_df['complete_text'] = news_df[\"headline\"].map(str)+ '. ' + news_df[\"article\"]\n",
        "\n",
        "# create a basic pre-processed corpus\n",
        "corpus = normalize_corpus(news_df['complete_text'], remove_digits=False)\n",
        "\n",
        "# Selecting a sample news article\n",
        "sentence = str(corpus[0:25])\n",
        "sentence_nlp = nlp(sentence)\n",
        "\n",
        "# POS tagging with Spacy \n",
        "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
        "pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type']).head(10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>POS tag</th>\n",
              "      <th>Tag type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[</td>\n",
              "      <td>-LRB-</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'</td>\n",
              "      <td>``</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>doctor</td>\n",
              "      <td>NN</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>find</td>\n",
              "      <td>VB</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>roundworm</td>\n",
              "      <td>JJ</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>japanese</td>\n",
              "      <td>JJ</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>woman</td>\n",
              "      <td>NN</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tonsil</td>\n",
              "      <td>NN</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>eat</td>\n",
              "      <td>VBP</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>sashimi</td>\n",
              "      <td>NN</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Word POS tag Tag type\n",
              "0          [   -LRB-    PUNCT\n",
              "1          '      ``    PUNCT\n",
              "2     doctor      NN     NOUN\n",
              "3       find      VB     VERB\n",
              "4  roundworm      JJ      ADJ\n",
              "5   japanese      JJ      ADJ\n",
              "6      woman      NN     NOUN\n",
              "7     tonsil      NN     NOUN\n",
              "8        eat     VBP     VERB\n",
              "9    sashimi      NN     NOUN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rzlaMzd-_o7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "2d4b47ad-3889-4281-9714-735a2822a9bc"
      },
      "source": [
        "c = Counter(([word.pos_ for word in sentence_nlp]))\n",
        "sbase = sum(c.values())\n",
        "for el, cnt in c.items():\n",
        "    print(el, cnt)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PUNCT 77\n",
            "NOUN 470\n",
            "VERB 185\n",
            "ADJ 140\n",
            "PROPN 232\n",
            "ADP 11\n",
            "NUM 96\n",
            "ADV 40\n",
            "X 2\n",
            "PART 4\n",
            "CCONJ 2\n",
            "PRON 4\n",
            "INTJ 2\n",
            "SCONJ 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwdkIc9zQBAD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "6c8b603e-3621-4291-97b3-50254d872ddd"
      },
      "source": [
        "Noun = []\n",
        "Pronoun = []\n",
        "Verb = []\n",
        "Adverb = []\n",
        "Conjunction = []\n",
        "Interjection = []\n",
        "Adjective = []\n",
        "Preposition = []\n",
        "\n",
        "for word in sentence_nlp:\n",
        "    if word.pos_ == \"NOUN\":\n",
        "        Noun.append(word)\n",
        "    elif word.pos_ == \"PRON\":\n",
        "        Pronoun.append(word)\n",
        "    elif word.pos_ == \"VERB\":\n",
        "        Verb.append(word)\n",
        "    elif word.pos_ == \"ADV\":\n",
        "        Adverb.append(word)\n",
        "    elif word.pos_ == \"INTJ\":\n",
        "        Interjection.append(word)\n",
        "    elif word.pos_ == \"ADJ\":\n",
        "        Adjective.append(word)\n",
        "    elif word.pos_ == \"ADP\":\n",
        "        Preposition.append(word)\n",
        "    elif word.pos_ == \"SCONJ\":\n",
        "        Conjunction.append(word)\n",
        "\n",
        "data = [['Noun', Noun, len(Noun)], ['Pronoun', Pronoun, len(Pronoun)],\n",
        "        ['Verb', Verb, len(Verb)], ['Adverb', Adverb, len(Adverb)],\n",
        "        ['Conjunction', Conjunction, len(Conjunction)],\n",
        "        ['Interjection', Interjection, len(Interjection)],\n",
        "        ['Adjective', Adjective, len(Adjective)],['Preposition', Preposition, len(Preposition)]]\n",
        "\n",
        "final_df = pd.DataFrame(data, columns = ['Part of Speech', 'Words', 'Word_Count'])\n",
        "final_df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Part of Speech</th>\n",
              "      <th>Words</th>\n",
              "      <th>Word_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Noun</td>\n",
              "      <td>[doctor, woman, tonsil, sashimi, pain, throat,...</td>\n",
              "      <td>470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pronoun</td>\n",
              "      <td>[us, someone, us, mine]</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Verb</td>\n",
              "      <td>[find, eat, 25year, visit, find, consume, shed...</td>\n",
              "      <td>185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Adverb</td>\n",
              "      <td>[early, almost, next, eventually, previously, ...</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Conjunction</td>\n",
              "      <td>[upon, whether, near]</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Interjection</td>\n",
              "      <td>[no, see]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Adjective</td>\n",
              "      <td>[roundworm, japanese, old, uncommon, assorted,...</td>\n",
              "      <td>140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Preposition</td>\n",
              "      <td>[inside, bioelectronic, aboard, journey, aroun...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Part of Speech                                              Words  Word_Count\n",
              "0           Noun  [doctor, woman, tonsil, sashimi, pain, throat,...         470\n",
              "1        Pronoun                            [us, someone, us, mine]           4\n",
              "2           Verb  [find, eat, 25year, visit, find, consume, shed...         185\n",
              "3         Adverb  [early, almost, next, eventually, previously, ...          40\n",
              "4    Conjunction                              [upon, whether, near]           3\n",
              "5   Interjection                                          [no, see]           2\n",
              "6      Adjective  [roundworm, japanese, old, uncommon, assorted,...         140\n",
              "7    Preposition  [inside, bioelectronic, aboard, journey, aroun...          11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfXNDXAKCBXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}